{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.5 64-bit ('Machine-Learning-Literature-Review': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d9554a4460f1b5015239dcb1bd401a6c61b0f03d7e0585cd274cbed4255a1eca"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Leakage from Gradients"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*文章已经放在资源文件中*，文件位置：resources\\literature\\federated learning\\fl-1.pdf"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Abstract\n",
    "\n",
    "1.\t人们认为共享梯度是安全的，事实上不是。\n",
    "2.  本文实验了利用共享梯度可以在 NLP 领域可以获得 token 级别的原始数据，在 CV 领域可以获得像素级别的数据。\n",
    "3.  讨论了几种避免这种数据泄露的策略。其中，在不改变训练设定的情况下，最有效的办法是梯度截断。\n",
    "4.  提高人们的安全意识。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Introduction\n",
    "\n",
    "### 分布式训练：\n",
    "\n",
    "- 在大规模数据集上可以加速训练过程。\n",
    "- 计算在各个 worker 上执行，通过交换梯度进行同步。\n",
    "- 数据是分散的，数据属于各个 worker，不同的 worker 只有在训练的过程中交换梯度。\n",
    "- 可以利用不同来源的数据共同训练一个模型。\n",
    "\n",
    "如果数据需要隐私保护的，这种模式叫做**协同训练**。\n",
    "\n",
    "### 共享梯度真的安全么？\n",
    "\n",
    "* 直觉：\n",
    "    - 共享梯度不会泄露原始数据。\n",
    "    - 给定学习模型 $F$ 、模型参数 $W$、共享梯度 $\\nabla w$ 和标签对，是**无法**反向得到训练数据的。\n",
    "* 实际：\n",
    "    - 可以利用共享梯度判断具有某些属性的数据是否存在于一批数据中。\n",
    "    - 通过 GAN 可以生成类似于接近于训练数据的数据。\n",
    "    - 给定学习模型 $F$ 、模型参数 $W$、共享梯度 $\\nabla w$ 和标签对，是**可以**反向得到训练数据的。\n",
    "\n",
    "### 文章的贡献\n",
    "\n",
    "* 提出并实现了 DLG 算法\n",
    "    - 快速，仅需要几次迭代，就可以通过公开的梯度获取原始训练数据和其标签对。\n",
    "    - 精确，可以获取像素级别或者 token 级别的训练数据。之前的方法只是近似的结果。\n",
    "    - 依赖少：\n",
    "        - 无需对训练数据做出假设\n",
    "        - 不依赖任何生成模型   \n",
    "* 分析了不同场景下攻击的难点。\n",
    "* 讨论了面对攻击的几种不同防卫策略：\n",
    "    - 梯度扰动：加入高斯和拉普拉斯噪声\n",
    "    - 低精度：精度降低一半\n",
    "    - 梯度压缩：梯度截断 20%"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Related Work\n",
    "\n",
    "### 分布式训练\n",
    "\n",
    "因为训练大量的数据是计算密集的，很费时间，所以有很多工作从算法和框架两个角度关注如何通过分布式训练加速训练过程，使得训练能在一个相对合理的时间内完成。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}